Meta-analyses in psychology often pool outcomes measured using different instruments, with many studies reporting only a subset of these outcomes. 
Such instruments typically measure the same underlying construct and are statistically correlated, making multivariate meta-analysis a methodologically appropriate choice. 
However, missing outcomes raise concerns about bias and inference validity, particularly when the mechanism driving missingness is unknown. 
While some outcomes may be Missing Completely at Random (MCAR) or Missing at Random (MAR), a more complex scenario arises when outcomes are Missing Not At Random (MNAR).

Missing study-level summary statistics are handled by repeated stochastic single imputation: in each Monte-Carlo replicate every absent effect estimate, standard error, 
and within-study correlation is replaced by a single random draw from one of four donor distributions—uniform, univariate normal, multivariate normal, 
or truncated normal—selected to represent a spectrum of plausible data-generating processes while preserving cross-outcome dependence where appropriate.
These imputations are embedded in a multiverse sensitivity design that systematically varies both the assumed missingness mechanism (MCAR, MAR, MNAR) 
and the meta-analytic specification across thousands of replicates.

The multiverse approach offers several advantages. It retains the observed correlation structure, prevents artificial distortion of heterogeneity, 
remains computationally lightweight, and makes the dependence of inferences on unverifiable assumptions fully transparent. 
Simulations show that misspecifying the mechanism (e.g., analysing MNAR data as MAR) yields biased pooled effects and under-coverage, 
whereas structurally coherent imputation models improve inferential accuracy. We conclude with practical recommendations and reproducible code for conducting 
multiverse sensitivity analyses in psychological MVMA when outcome-level missingness and uncertainty about its mechanism are present.




Meta-analyses in psychology often pool outcomes measured using different instruments, with many studies reporting only a subset of these outcomes. 
Such instruments might measure the same underlying construct and be statistically correlated, making multivariate meta-analysis a methodologically appropriate choice. 
However, missing outcomes raise concerns about bias and inference validity, particularly when the missingness mechanism is unknown. 
While some outcomes may be Missing Completely at Random (MCAR) or Missing at Random (MAR), a more complex scenario arises when outcomes are Missing Not At Random (MNAR).

We propose a strategy for addressing missing study-level summary statistics using repeated stochastic single imputation. 
Within each Monte Carlo replicate, missing effect estimates, standard errors, and within-study correlations are replaced by a single random draw from predefined donor distributions 
(e.g., uniform, univariate normal, multivariate normal), to capture a range of plausible data-generating mechanisms. 
By doing so, we systematically explore the interaction between assumptions about the missingness mechanism (MCAR, MAR, MNAR) and alternative meta-analytic specifications across thousands of replicates.
The proposed approach has two main advantages: it is computationally efficient and, most importantly, makes explicit the assumptions underpinning the analysis rather than relying solely on the data. 
This transparency allows researchers to clearly articulate and test the implications of their assumptions, including the use of delta-adjustments for sensitivity analyses. 
We conclude by offering practical guidelines and open-source code to facilitate sensitivity analyses in psychological MVMA.







